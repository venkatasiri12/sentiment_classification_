{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68526671",
   "metadata": {},
   "source": [
    "# 05_train_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43fdac",
   "metadata": {},
   "source": [
    "Fine-tune DistilBERT (or RoBERTa) using Hugging Face Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffcc26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment_clean/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/sentiment_clean/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/anaconda3/envs/sentiment_clean/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.36.2\n",
      "accelerate: 0.25.0\n",
      "datasets: 2.14.5\n"
     ]
    }
   ],
   "source": [
    "import transformers, accelerate, datasets\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3212af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:19<00:00, 1292.23 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:22<00:00, 1106.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:46<00:00, 1080.93 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                  \n",
      " 20%|â–ˆâ–‰        | 99/500 [14:11<1:03:57,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1053, 'learning_rate': 4e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–‰        | 99/500 [15:10<1:03:57,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'learning_rate': 3e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–‰        | 99/500 [16:09<1:03:57,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–‰        | 99/500 [17:15<1:03:57,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–‰        | 99/500 [18:36<1:03:57,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Downloading builder script: 4.20kB [00:00, 1.37MB/s]\n",
      "                                                  \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      " 20%|â–ˆâ–‰        | 99/500 [19:02<1:03:57,  9.57s/it]\n",
      "\u001b[A\n",
      "                                                  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:51<00:00,  3.51s/it]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0004960223450325429, 'eval_accuracy': 1.0, 'eval_runtime': 26.2221, 'eval_samples_per_second': 7.627, 'eval_steps_per_second': 0.953, 'epoch': 1.0}\n",
      "{'train_runtime': 351.1819, 'train_samples_per_second': 2.278, 'train_steps_per_second': 0.285, 'train_loss': 0.021903112158179285, 'epoch': 1.0}\n",
      "\n",
      "ðŸŽ‰ Training complete! Model saved to: artifacts/transformer\n"
     ]
    }
   ],
   "source": [
    "# FAST + CLEAN TRAINING CODE FOR DISTILBERT\n",
    "\n",
    "# Required packages (already installed):\n",
    "# transformers==4.36.2\n",
    "# datasets==2.14.5\n",
    "# accelerate\n",
    "# evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "out = \"artifacts/transformer\"\n",
    "os.makedirs(out, exist_ok=True)\n",
    "\n",
    "# 1) Load IMDB dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "\n",
    "# 2) Load tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3) Preprocess function\n",
    "def preprocess(x):\n",
    "    return tok(\n",
    "        x[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128    # shorter = faster\n",
    "    )\n",
    "\n",
    "# Apply tokenizer\n",
    "tds = ds.map(preprocess, batched=True)\n",
    "\n",
    "# Cleanup columns and format\n",
    "tds = (\n",
    "    tds.remove_columns([\"text\"])\n",
    "    .rename_column(\"label\", \"labels\")\n",
    "    .with_format(\"torch\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ðŸ”¥ REDUCED TRAINING SET (MUCH FASTER)\n",
    "# ----------------------------------------------------\n",
    "train_small = tds[\"train\"].select(range(800))   # reduced from 25,000\n",
    "eval_small  = tds[\"test\"].select(range(200))\n",
    "\n",
    "# 4) Load pretrained DistilBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 5) Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    ")\n",
    "\n",
    "# 6) Metric function\n",
    "def compute_metrics(p):\n",
    "    import evaluate\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# 7) Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_small,\n",
    "    eval_dataset=eval_small,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 8) Train\n",
    "trainer.train()\n",
    "\n",
    "# 9) Save model\n",
    "trainer.save_model(out)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training complete! Model saved to:\", out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
